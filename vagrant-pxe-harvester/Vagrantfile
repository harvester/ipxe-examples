# vi: set ft=ruby ts=2 :

require 'yaml'

VAGRANTFILE_API_VERSION = "2"

# check for required plugins
_required_plugins_list = %w{vagrant-libvirt}
exit(1) unless _required_plugins_list.all? do |plugin|
  Vagrant.has_plugin?(plugin) || (
    STDERR.puts "Required plugin '#{plugin}' is missing; please install using:"
    STDERR.puts "  % vagrant plugin install #{plugin}"
    false
  )
end

# ensure libvirt is the default provider in case the vagrant box config
# doesn't specify it
ENV['VAGRANT_DEFAULT_PROVIDER'] = "libvirt"

@root_dir = File.dirname(File.expand_path(__FILE__))
@settings = YAML.load_file(File.join(@root_dir, "settings.yml"))

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|

  # containerd is taking more than 60 seconds to shutdown in SUSE platforms
  # so increase the timeout to 120 seconds
  config.vm.graceful_halt_timeout = 120
  
  # PXE Server
  config.vm.define :pxe_server do |pxe_server|
    pxe_server.vm.box = 'generic/debian11'
    pxe_server.vm.hostname = 'pxe-server'
    pxe_server.vm.network 'private_network',
      ip: @settings['harvester_network_config']['dhcp_server']['ip'],
      libvirt__network_name: 'harvester',
      # don't enable DHCP as this node will have it's now DHCP server for iPXE
      libvirt__dhcp_enabled: false

    pxe_server.vm.provider :libvirt do |libvirt|
      libvirt.cpu_mode = 'host-passthrough'

      # If S3 backup target is enabled, allocate a little more CPU and memory
      # and create a suitable storage volume
      if @settings['s3']['enabled']
        libvirt.memory = '4096'
        libvirt.cpus = '2'
        libvirt.storage :file,
          size: @settings['s3']['capacity'],
          type: 'qcow2',
          bus: 'virtio',
          device: 'vdb'
      else
        libvirt.memory = '1024'
        libvirt.cpus = '1'
      end
    end

    pxe_server.vm.provision :ansible do |ansible|
      ansible.playbook = 'ansible/setup_pxe_server.yml'
      ansible.extra_vars = {
        settings: @settings
      }
    end
  end

  # Harvester
  cluster_node_index = @settings['harvester_cluster_nodes'] - 1
  (0..cluster_node_index).each do |node_number|
    vm_name = "harvester-node-#{node_number}"
    config.vm.define vm_name, autostart: false do |harvester_node|
      harvester_node.vm.hostname = "harvester-node-#{node_number}"
      harvester_node.vm.network 'private_network',
        libvirt__network_name: 'harvester',
        mac: @settings['harvester_network_config']['cluster'][node_number]['mgmt_mac']

      harvester_node.ssh.host = "#{@settings['harvester_network_config']['cluster'][node_number]['ip']}"
      harvester_node.ssh.username = "rancher"

      harvester_node.vm.provider :libvirt do |libvirt|
        libvirt.qemu_use_agent = true
        libvirt.mgmt_attach = false
        libvirt.cpu_mode = 'host-passthrough'
        libvirt.memory = @settings['harvester_network_config']['cluster'][node_number].key?('memory') ? @settings['harvester_network_config']['cluster'][node_number]['memory'] : @settings['harvester_node_config']['memory'] 
        libvirt.cpus = @settings['harvester_network_config']['cluster'][node_number].key?('cpu') ? @settings['harvester_network_config']['cluster'][node_number]['cpu'] : @settings['harvester_node_config']['cpu']
        libvirt.storage :file,
          size: @settings['harvester_network_config']['cluster'][node_number].key?('disk_size') ? @settings['harvester_network_config']['cluster'][node_number]['disk_size'] : @settings['harvester_node_config']['disk_size'],
          type: 'qcow2',
          bus: 'virtio',
          device: 'vda'
        boot_network = {'network' => 'harvester'}
        libvirt.boot 'hd'
        libvirt.boot boot_network

        os_release = `cat /etc/os-release`.downcase
        loader_path = if os_release.include?("ubuntu")
            "/usr/share/qemu/OVMF.fd"
        else
            "/usr/share/qemu/ovmf-x86_64.bin"
        end
        # NOTE: default to UEFI boot. Comment this out for legacy BIOS.
        libvirt.loader = loader_path
        libvirt.nic_model_type = 'e1000'

        # Any node for which nvme_size is specified in settings.yml will
        # automatically have one additional virtual NVMe disk created,
        # which will appear as /dev/nvme0n1.  This can be useful for
        # testing additional disks with Longhorn.  The way it works
        # is a bit obscure because libvirt only seems to support virtio,
        # scsi and sata disks, so we have to resort to hacking qemu
        # arguments in as described in this blog post:
        #
        #   http://blog.frankenmichl.de/2018/02/13/add-nvme-device-to-vm/
        #
        # This means we have to go create the volume manually first with
        # `virsh` then pass its path to qemu.  Matters are complicated
        # further by volumes being owned by root by default, while libvirt
        # runs VMs as the qemu user.  For normal libvirt volumes, file
        # ownership changes happen automatically, but for these volumes
        # we're hacking in by hand, that doesn't happen, so we have to
        # explicitly specify ownership, and to do _that_ we have to
        # define the volume via XML, hence the nasty `virsh vol-create-as
        # ... | sed ... | virsh vol-create` invocation.
        #
        # The other wrinkle is that we need to know the exact path to
        # the disk image.  Once a volume has been created, you can run
        # `virsh vol-path --pool default VOLUME_NAME` to get the path,
        # but we need to know the path when setting libvirt.qemuargs,
        # whereas we don't want to actually create the volume until the
        # `vagrant up` trigger for the node in question.  If we create
        # the volume outside the trigger, it gets created on _every_
        # evaluation of the Vagrant file, even for unrelated VMs...
        # So, we call `virsh pool-dumpxml default` and get the pool
        # path from that, then stick the volume name on the end.
        if @settings['harvester_network_config']['cluster'][node_number].key?('nvme_size')

          nvme0_name = "#{File.basename(@root_dir)}_#{vm_name}-nvme0.qcow2"
          require 'nokogiri'
          pool_path = Nokogiri::XML(%x(virsh -c qemu:///system pool-dumpxml default)).at_xpath('/pool/target/path').content
          nvme0_path = File.join(pool_path, nvme0_name)

          require 'etc'
          storage_owner = begin
            Etc.getpwnam('qemu')
          rescue ArgumentError
            # Just in case the qemu user doesn't exist, fall back to root
            # and hope it works :-/
            Etc.getpwnam('root')
          end

          libvirt.qemuargs :value => '-drive'
          libvirt.qemuargs :value => "file=#{nvme0_path},if=none,id=nvme0"
          libvirt.qemuargs :value => '-device'
          # "addr 10" below is arbitrary and is intended to avoid the following
          # error which occurs if addr is not specified:
          #   /usr/lib64/ruby/gems/2.5.0/gems/fog-libvirt-0.7.0/lib/fog/libvirt/requests/compute/vm_action.rb:7:
          #   in `create': Call to virDomainCreateWithFlags failed: internal error:
          #   process exited while connecting to monitor: 2024-11-01T05:46:54.478063Z
          #   qemu-system-x86_64: -device {"driver":"cirrus-vga","id":"video0","bus":
          #   "pci.0","addr":"0x2"}: PCI: slot 2 function 0 not available for cirrus-vga,
          #   in use by nvme,id=(null) (Libvirt::Error)
          libvirt.qemuargs :value => "nvme,addr=10,drive=nvme0,serial=#{node_number}_1234"

          harvester_node.trigger.before :up do |trigger|
            trigger.warn = "Creating volume #{nvme0_name}"
            # `virsh vol-info ... ` at the start ensures we skip the call to
            # `virsh vol-create` if the volume already exists
            trigger.run = {inline: "sh -c \"\
              export LIBVIRT_DEFAULT_URI=qemu:///system ; \
              virsh vol-info --pool=default #{nvme0_name} >/dev/null 2>&1 || \
              virsh vol-create-as default #{nvme0_name} \
                    --capacity #{@settings['harvester_network_config']['cluster'][node_number]['nvme_size']} \
                    --format qcow2 --print-xml | \
              sed '/<target>/a <permissions><owner>#{storage_owner.uid}</owner><group>#{storage_owner.gid}</group></permissions>' | \
              virsh vol-create default /dev/stdin 2>&1\""}
          end

          harvester_node.trigger.after :destroy do |trigger|
            trigger.warn = "Destroying volume #{nvme0_name}"
            # `|| :`` on the end here means that we don't abort the Vagrant
            # run even if the volume doesn't exist or deletion fails (an error
            # will still be logged to the console)
            trigger.run = {inline: "sh -c \"virsh -c qemu:///system vol-delete --pool default #{nvme0_name} || :\""}
          end
        end
      end
    end
  end

  # Rancher
  if @settings['rancher_config']['enabled']
    config.vm.define :rancher do |rancher|
      rancher.vm.box = 'generic/debian10'
      rancher.vm.hostname = 'rancher'
      rancher.vm.network 'private_network',
        ip: @settings['rancher_config']['ip'],
        libvirt__network_name: 'harvester',
        libvirt__dhcp_enabled: false

        rancher.vm.provider :libvirt do |libvirt|
        libvirt.cpu_mode = 'host-passthrough'
        libvirt.cpus = @settings['rancher_config']['cpu']
        libvirt.memory = @settings['rancher_config']['memory']
      end

      rancher.vm.provision :ansible do |ansible|
        ansible.playbook = 'ansible/setup_rancher.yml'
        ansible.extra_vars = {
          settings: @settings
        }
      end
    end
  end

end
