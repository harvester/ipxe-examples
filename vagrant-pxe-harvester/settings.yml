---
##########################################################################
# NOTE: this is a YAML file so please pay close attention to the leading #
# spaces as they are significant.                                        #
##########################################################################

#
# harvester_iso_url
# harvester_kernel_url
# harvester_initrd_url
#
# Harvester media to install. The URL scheme can be either 'http', 'https', or
# 'file'. If the URL scheme is 'file', the given media will be copied from the
# local file system instead of downloading from a remote location.
harvester_iso_url: https://releases.rancher.com/harvester/master/harvester-master-amd64.iso
harvester_kernel_url: https://releases.rancher.com/harvester/master/harvester-master-vmlinuz-amd64
harvester_ramdisk_url: https://releases.rancher.com/harvester/master/harvester-master-initrd-amd64
harvester_rootfs_url: https://releases.rancher.com/harvester/master/harvester-master-rootfs-amd64.squashfs

#
# harvester_cluster_nodes
#
# NOTE: keep in mind that you need at least 3 nodes to make a cluster
#
harvester_cluster_nodes: 1

#
# network_config
#
# Harvester network configurations. Make sure the cluster IPs are on the same
# subnet as the DHCP server. Pre-assign the IPs and MACs for the Harvester
# nodes.
#
# NOTE: Random MAC addresses are generated with the following command:
# printf '02:00:00:%02X:%02X:%02X\n' $((RANDOM%256)) $((RANDOM%256)) $((RANDOM%256))
# Thanks to https://stackoverflow.com/questions/8484877/mac-address-generator-in-python
# If any of the generated MAC addresses is in conflict with an existing one in
# your environment, please use the above command to regenerate and replace
# the conflicting one.

harvester_network_config:
  # Run as an airgapped environment that only has internet connectivity through an HTTP proxy.
  # The HTTP proxy runs on DHCP server using port 3128
  offline: true

  dhcp_server:
    ip: 192.168.0.254
    subnet: 192.168.0.0
    netmask: 255.255.255.0
    range: 192.168.0.50 192.168.0.130
    https: false
  # Reserve these IPs for the Harvester cluster. Make sure these are outside
  # the range of DHCP so they don't get served out by the DHCP server
  # The Harvester cluster IPs are also represented in the 'inventory' file, so editing these
  # you would also want to make updates / edits to the inventory file
  vip:
    ip: 192.168.0.131
    mode: DHCP
    mac: 02:00:00:03:3D:61
  cluster:
    - ip: 192.168.0.30
      mac: 02:00:00:0D:62:E2
      cpu: 6
      memory: 16384
      disk_size: 150G
      vagrant_interface: ens5
      mgmt_interface: ens6
    - ip: 192.168.0.31
      mac: 02:00:00:35:86:92
      cpu: 4
      memory: 8192
      disk_size: 150G
      vagrant_interface: ens5
      mgmt_interface: ens6
    - ip: 192.168.0.32
      mac: 02:00:00:2F:F2:2A
      cpu: 6
      memory: 16384
      disk_size: 150G
      vagrant_interface: ens5
      mgmt_interface: ens6
    - ip: 192.168.0.33
      mac: 02:00:00:A7:E6:FF
      cpu: 4
      memory: 8192
      disk_size: 150G
      vagrant_interface: ens5
      mgmt_interface: ens6

#
# harvester_config
#
# Harvester system configurations.
#
harvester_config:
  # static token for cluster authentication
  token: token

  # Public keys to add to authorized_keys of each node.
  ssh_authorized_keys:
    # Vagrant default unsecured SSH public key
    - ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key

  # password to for the `rancher` user to login to the Harvester nodes
  password: p@ssword

  # NTP servers
  ntp_servers:
    - 0.suse.pool.ntp.org
    - 1.suse.pool.ntp.org

#
# rancher_config
# 
# Rancher configurations
# 
rancher_config:
  # disk size of single instance rancher node, this is split into two partitions at 50/50
  node_disk_size: 350G
  # to determine to run air_gapped rancher, future state, may have non-airgapped supported
  run_single_node_air_gapped_rancher: true
  # cert-manager version, for the jetstack.io repo
  cert_manager_version: v1.7.1
  # url escaped k3s version for grabbing k3s
  k3s_url_escaped_version: v1.23.4%2Bk3s1
  # the rancher version with it's prefix
  rancher_version: v2.6.4-rc13
  # the rancher version without it's prefix
  rancher_version_no_prefix: 2.6.4-rc13
  # k9s_version: v0.25.18
  # mac address of the harvester network card for dhcp on harvester network to work 
  mac_address_harvester_network: 02:29:F9:43:92:95
  # if this ip changes, update it in the inventories/vagrant file for the rancher_box
  # we interact with the libvirt vm on this IP via ansible, then shut off eth0 which provides a temp network out
  # you'll also need to change the ip, listed in the DHCP configuration, since 192.168.0.34 is directly tied to it
  node_harvester_network_ip: 192.168.0.34
  cpu: 6
  memory: 16384
  # rancher_install_domain, where the domain of the rancher install via helm templating and kubectl -R -f applying is 
  rancher_install_domain: helm-install.test
  # registry_domain is the docker domain that get's set up, it can be anything as long as it ends in a ".local"
  registry_domain: myregistry.local
  # this is the coredns build image that harvester uses change when needed
  harvester_coredns_build: rancher/hardened-coredns:v1.9.1-build20220318


#
# harvester_node_config
#
# Harvester node-specific configurations.
#
harvester_node_config:
  # number of CPUs assigned to each node
  cpu: 6

  # memory size for each node, in MBytes
  memory: 16384

  # disk size for each node
  disk_size: 100G
